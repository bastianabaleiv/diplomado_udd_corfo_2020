{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wykx-ARBFoz0"
   },
   "source": [
    "# Regresion Lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XL-XgV7PFoz3"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6TcygL3Fo0A"
   },
   "source": [
    "## Regresion Lineal Simple\n",
    "\n",
    "Comenzaremos con la regresión lineal más familiar, un ajuste de línea recta a los datos.\n",
    "Un ajuste en línea recta es un modelo de la forma\n",
    "$$\n",
    "y = ax + b\n",
    "$$\n",
    "donde $a$ se conoce comúnmente como *pendiente*, y $b$ se conoce comúnmente como *intersección*.\n",
    "\n",
    "Considere los siguientes datos, que se encuentran dispersos sobre una línea con una pendiente de 2 y una intersección de -5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cZrvZpZMFo0C",
    "outputId": "0f49d247-ac3c-44ee-c1a4-aba2e1a4125a"
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "x = 10 * rng.rand(50)\n",
    "y = 2 * x - 5 + rng.randn(50)\n",
    "plt.scatter(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_WjA97Q-Fo0I"
   },
   "source": [
    "Usaremos el estimador de Scikit-Learn ``LinearRegression`` para ajustar la data y construir el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AiEE7jvUFo0K",
    "outputId": "e493506c-9cc1-4fd6-a38b-d332943d9abf"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "\n",
    "model.fit(x[:, np.newaxis], y)\n",
    "\n",
    "xfit = np.linspace(0, 10, 1000)\n",
    "yfit = model.predict(xfit[:, np.newaxis])\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(xfit, yfit);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZvOlKg_Fo0V"
   },
   "source": [
    "La pendiente y la intersección de los datos están contenidos en los parámetros de ajuste del modelo, que en Scikit-Learn siempre están marcados con un guión bajo al final.\n",
    "Aquí los parámetros relevantes son `` coef_`` e `` intercept_``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TSAoSyAgFo0V",
    "outputId": "257dfbd6-4a3c-4e19-e024-637bff84327c"
   },
   "outputs": [],
   "source": [
    "print(\"Pendiente:    \", model.coef_[0])\n",
    "print(\"Intercepto:\", model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ca2jDSGyFo0b"
   },
   "source": [
    "Vemos que los resultados están muy cerca de las entradas, como podríamos esperar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fR_T54yFo0c"
   },
   "source": [
    "Sin embargo, el estimador de ``LinearRegression`` es mucho más capaz que esto; además de ajustes simples en línea recta, también puede manejar modelos lineales multidimensionales de la forma\n",
    "$$\n",
    "y = a_0 + a_1 x_1 + a_2 x_2 + \\cdots\n",
    "$$\n",
    "donde hay multiples valores $x$.\n",
    "Geométricamente, esto es similar a ajustar un plano a puntos en tres dimensiones, o ajustar un hiperplano a puntos en dimensiones más altas.\n",
    "\n",
    "La naturaleza multidimensional de tales regresiones las hace más difíciles de visualizar, pero podemos ver uno de estos ajustes en acción construyendo algunos datos de ejemplo, usando el operador de multiplicación de matrices de NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7wzl118gFo0c",
    "outputId": "84669000-e23a-45cb-bd42-0c772852c0f7"
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "X = 10 * rng.rand(100, 3)\n",
    "y = 0.5 + np.dot(X, [1.5, -2., 1.])\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "model.fit(X, y)\n",
    "print(model.intercept_)\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QijrHke6Fo0i"
   },
   "source": [
    "Aquí, los datos $ y $ se construyen a partir de tres valores $ x $ aleatorios, y la regresión lineal recupera los coeficientes utilizados para construir los datos.\n",
    "\n",
    "De esta manera, podemos usar el estimador único de ``LinearRegression``  para ajustar líneas, planos o hiperplanos a nuestros datos.\n",
    "Todavía parece que este enfoque se limitaría a relaciones estrictamente lineales entre variables, pero resulta que también podemos relajar esto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vy4apwL4Fo0i"
   },
   "source": [
    "## Regresion de funciones base\n",
    "\n",
    "Un truco que puede utilizar para adaptar la regresión lineal a relaciones no lineales entre variables es transformar los datos de acuerdo con *funciones base*.\n",
    "\n",
    "La idea es tomar nuestro modelo lineal multidimensional:\n",
    "$$\n",
    "y = a_0 + a_1 x_1 + a_2 x_2 + a_3 x_3 + \\cdots\n",
    "$$\n",
    "y construir $x_1, x_2, x_3,$, de nuestra entrada unidimiensional $x$.\n",
    "Esto es, $x_n = f_n(x)$, donde $f_n()$ es una funcion que transforma la data.\n",
    "\n",
    "Por ejemplo, si $f_n(x) = x^n$, nuestro modelo se transforma en una regresion polinomial:\n",
    "$$\n",
    "y = a_0 + a_1 x + a_2 x^2 + a_3 x^3 + \\cdots\n",
    "$$\n",
    "Observe que este es *todavía un modelo lineal*: la linealidad se refiere al hecho de que los coeficientes $a_n$ nunca se multiplican ni se dividen entre sí.\n",
    "Lo que hemos hecho efectivamente es tomar nuestros valores unidimensionales $x$ y proyectarlos en una dimensión superior, de modo que un ajuste lineal pueda encajar relaciones más complicadas entre $x$ y $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ke35C52jFo0j"
   },
   "source": [
    "### Funciones de base polinomial\n",
    "\n",
    "Esta proyección polinomial es lo suficientemente útil como para estar integrada en Scikit-Learn, utilizando el transformador ``PolynomialFeatures``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "de3bm9ADFo0k",
    "outputId": "99f5b5d9-9c31-4de1-9353-2b5edf1950c2"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "x = np.array([2, 3, 4])\n",
    "poly = PolynomialFeatures(3, include_bias=False)\n",
    "poly.fit_transform(x[:, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Snn3ghbdFo0o"
   },
   "source": [
    "Vemos aquí que el transformador ha convertido nuestra matriz unidimensional en una matriz tridimensional tomando el exponente de cada valor.\n",
    "Esta nueva representación de datos de mayor dimensión se puede conectar a una regresión lineal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ZOwS-NEFo0p"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "poly_model = make_pipeline(PolynomialFeatures(7),\n",
    "                           LinearRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wF-zQ2p7Fo0t"
   },
   "source": [
    "Con esta transformación en su lugar, podemos usar el modelo lineal para ajustar relaciones mucho más complicadas entre $x$ y $y$.\n",
    "Por ejemplo, aquí hay una onda sinusoidal con ruido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "__pkfp5xFo0v",
    "outputId": "6c92c75a-04db-4d2e-d28f-945eb9a3f90e"
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "x = 10 * rng.rand(50)\n",
    "y = np.sin(x) + 0.1 * rng.randn(50)\n",
    "\n",
    "poly_model.fit(x[:, np.newaxis], y)\n",
    "yfit = poly_model.predict(xfit[:, np.newaxis])\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(xfit, yfit);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gibmXmm1Fo0y"
   },
   "source": [
    "Nuestro modelo lineal, mediante el uso de funciones de base polinomial de séptimo orden, puede proporcionar un ajuste excelente a estos datos no lineales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5ZjcDLiFo0z"
   },
   "source": [
    "### Funciones base gaussianas\n",
    "Por supuesto, son posibles otras funciones básicas.\n",
    "Por ejemplo, un patrón útil es ajustar un modelo que no es una suma de bases polinomiales, sino una suma de bases gaussianas.\n",
    "El resultado podría parecerse a la siguiente figura:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqet5JpUFo00"
   },
   "source": [
    "![](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/figures/05.06-gaussian-basis.png?raw=1)\n",
    "[figure source in Appendix](#Gaussian-Basis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqy0Oq-qFo00"
   },
   "source": [
    "Las regiones sombreadas en el gráfico son las funciones base escaladas y, cuando se suman, reproducen la curva suave a través de los datos.\n",
    "Estas funciones de base gaussiana no están integradas en Scikit-Learn, pero podemos escribir un transformador personalizado que las creará, como se muestra aquí y se ilustra en la siguiente figura (los transformadores de Scikit-Learn se implementan como clases de Python; leer la fuente de Scikit-Learn es una buena forma de ver cómo se pueden crear):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tSLfNLtXFo01",
    "outputId": "3a12fde6-0daa-4aa4-91e0-086b49502b70"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class GaussianFeatures(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Uniformly spaced Gaussian features for one-dimensional input\"\"\"\n",
    "    \n",
    "    def __init__(self, N, width_factor=2.0):\n",
    "        self.N = N\n",
    "        self.width_factor = width_factor\n",
    "    \n",
    "    @staticmethod\n",
    "    def _gauss_basis(x, y, width, axis=None):\n",
    "        arg = (x - y) / width\n",
    "        return np.exp(-0.5 * np.sum(arg ** 2, axis))\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # create N centers spread along the data range\n",
    "        self.centers_ = np.linspace(X.min(), X.max(), self.N)\n",
    "        self.width_ = self.width_factor * (self.centers_[1] - self.centers_[0])\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return self._gauss_basis(X[:, :, np.newaxis], self.centers_,\n",
    "                                 self.width_, axis=1)\n",
    "    \n",
    "gauss_model = make_pipeline(GaussianFeatures(20),\n",
    "                            LinearRegression())\n",
    "gauss_model.fit(x[:, np.newaxis], y)\n",
    "yfit = gauss_model.predict(xfit[:, np.newaxis])\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(xfit, yfit)\n",
    "plt.xlim(0, 10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20AwZpVnFo06"
   },
   "source": [
    "Ponemos este ejemplo aquí solo para aclarar que no hay nada mágico en las funciones de base polinómica: si tiene algún tipo de intuición en el proceso de generación de sus datos que le hace pensar que una base u otra podría ser apropiada, puede usarlas como bien."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lPDAg6daFo07"
   },
   "source": [
    "## Regularization\n",
    "\n",
    "La introducción de funciones base en nuestra regresión lineal hace que el modelo sea mucho más flexible, pero también puede conducir muy rápidamente a un ajuste excesivo.\n",
    "Por ejemplo, si elegimos demasiadas funciones de base gaussiana, terminamos con resultados que no se ven tan bien:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ooj7EXLAFo08",
    "outputId": "ee292d8e-05f7-422e-95f2-33550fe3dd41"
   },
   "outputs": [],
   "source": [
    "model = make_pipeline(GaussianFeatures(30),\n",
    "                      LinearRegression())\n",
    "model.fit(x[:, np.newaxis], y)\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(xfit, model.predict(xfit[:, np.newaxis]))\n",
    "\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(-1.5, 1.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oD24wzl1Fo0-"
   },
   "source": [
    "Con los datos proyectados a la base de 30 dimensiones, el modelo tiene demasiada flexibilidad y llega a valores extremos entre ubicaciones donde está limitado por los datos.\n",
    "Podemos ver la razón de esto si graficamos los coeficientes de las bases gaussianas con respecto a sus ubicaciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q2nTwafcFo1B",
    "outputId": "b749b964-6bba-4258-adab-28b545b61e23"
   },
   "outputs": [],
   "source": [
    "def basis_plot(model, title=None):\n",
    "    fig, ax = plt.subplots(2, sharex=True)\n",
    "    model.fit(x[:, np.newaxis], y)\n",
    "    ax[0].scatter(x, y)\n",
    "    ax[0].plot(xfit, model.predict(xfit[:, np.newaxis]))\n",
    "    ax[0].set(xlabel='x', ylabel='y', ylim=(-1.5, 1.5))\n",
    "    \n",
    "    if title:\n",
    "        ax[0].set_title(title)\n",
    "\n",
    "    ax[1].plot(model.steps[0][1].centers_,\n",
    "               model.steps[1][1].coef_)\n",
    "    ax[1].set(xlabel='basis location',\n",
    "              ylabel='coefficient',\n",
    "              xlim=(0, 10))\n",
    "    \n",
    "model = make_pipeline(GaussianFeatures(30), LinearRegression())\n",
    "basis_plot(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwEFnsYOFo1F"
   },
   "source": [
    "El panel inferior de esta figura muestra la amplitud de la función base en cada ubicación.\n",
    "Este es un comportamiento de sobreajuste típico cuando las funciones base se superponen: los coeficientes de las funciones base adyacentes explotan y se cancelan entre sí.\n",
    "Sabemos que tal comportamiento es problemático, y sería bueno si pudiéramos limitar explícitamente tales picos en el modelo penalizando los valores grandes de los parámetros del modelo.\n",
    "Esta penalización se conoce como *regularización* y se presenta en varias formas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8-FaSptFo1G"
   },
   "source": [
    "### Ridge regression ($L_2$ Regularization)\n",
    "\n",
    "Quizás la forma más común de regularización se conoce como *ridge regression* o $ L_2 $ *regularización*, a veces también llamada *regularización de Tikhonov*.\n",
    "Esto procede penalizando la suma de cuadrados (2-normas) de los coeficientes del modelo; en este caso, la penalización en el ajuste del modelo sería\n",
    "$$\n",
    "P = \\alpha\\sum_{n=1}^N \\theta_n^2\n",
    "$$\n",
    "donde $\\alpha$ es un parametro libre que controla la fuerza de la penalidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iOGfxW9aFo1H",
    "outputId": "03c2da0c-0682-4907-a66f-5194caaeb4fb"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "model = make_pipeline(GaussianFeatures(30), Ridge(alpha=0.1))\n",
    "basis_plot(model, title='Ridge Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtRxIEHNFo1J"
   },
   "source": [
    "El parámetro $\\alpha$ es esencialmente una perilla que controla la complejidad del modelo resultante.\n",
    "En el límite $\\alpha\\ \\to 0$, recuperamos el resultado de regresión lineal estándar; en el límite $\\alpha \\to \\infty$, se suprimirán todas las respuestas del modelo.\n",
    "Una ventaja de la regresión de crestas en particular es que se puede calcular de manera muy eficiente, a un costo computacional apenas mayor que el modelo de regresión lineal original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOtctO-6Fo1L"
   },
   "source": [
    "### Lasso regression ($L_1$ regularization)\n",
    "\n",
    "Otro tipo de regularización muy común se conoce como Lasso, e implica penalizar la suma de valores absolutos (1-norma) de los coeficientes de regresión:\n",
    "$$\n",
    "P = \\alpha\\sum_{n=1}^N |\\theta_n|\n",
    "$$\n",
    "Aunque esto es conceptualmente muy similar a la regresión de crestas, los resultados pueden diferir sorprendentemente: por ejemplo, debido a razones geométricas, la regresión de lazo tiende a favorecer *modelos dispersos* cuando es posible: es decir, preferentemente establece los coeficientes del modelo exactamente a cero.\n",
    "\n",
    "Podemos ver este comportamiento al duplicar la figura de regresión de la cresta, pero usando coeficientes normalizados L1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z71tG1yKFo1L",
    "outputId": "6b740ea4-da80-4609-e135-1c26d9ae6e63"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "model = make_pipeline(GaussianFeatures(30), Lasso(alpha=0.001))\n",
    "basis_plot(model, title='Lasso Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3uryXxF0Fo1P"
   },
   "source": [
    "Con la penalización por regresión de lazo, la mayoría de los coeficientes son exactamente cero, y el comportamiento funcional está modelado por un pequeño subconjunto de las funciones básicas disponibles.\n",
    "Al igual que con la regularización de crestas, el parámetro $\\alpha$ ajusta la fuerza de la penalización y debe determinarse mediante, por ejemplo, validación cruzada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otro Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manera convencional para incluir la libreria\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lee un archivo CSV directamente de una URL y guarda los resultados\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/buruzaemon/me-ml/master/ISLR/Data/Advertising.csv', index_col=0)\n",
    "# Muestra las 5 primeras filas\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tipos de objetos principales:\n",
    "\n",
    "- **DataFrame:** filas y columnas (Como una planilla de cálculo)\n",
    "- **Series:** una sola columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestra las 5 últimas filas\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para chequear la forma de la matriz\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cuáles son las características?\n",
    "\n",
    "- **TV:** miles de dolares gastados en publicidad por TV por un producto.\n",
    "- **Radio:** miles de dolares gastados en Radio\n",
    "- **Newspaper:** miles de dolares gastados en Diarios escritos\n",
    "\n",
    "¿Cuál es la variable respuesta?\n",
    "- **Sales:** ventas del producto (miles)\n",
    "\n",
    "¿Qué más sabemos?\n",
    "- Debido a que la variable respuesta es contínua, es un problema de regresión.\n",
    "- Hay 200 observaciones (filas), y cada una es un mercado único."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizando la data usando seaborn\n",
    "\n",
    "**Seaborn:** Libreria de visualización de datos construida sobre Matplotlib\n",
    "\n",
    "- Anaconda: ejecutar **`conda install seaborn`** en una ventana de comando\n",
    "- Otros usuario: [installation instructions](http://stanford.edu/~mwaskom/software/seaborn/installing.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manera convencional de importar seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "# Permitir que los graficos aparezcan en las celdas\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar la relación entre las caracteristitas y la respuesta usando scatterplots\n",
    "sns.pairplot(data, x_vars=['TV','Radio','Newspaper'], y_vars='Sales', size=7, aspect=0.7, kind='reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data, kind=\"reg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr=data.corr()\n",
    "ax = sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión lineal\n",
    "\n",
    "**Ventajas:** Rápido, no requiere ajustes, altamente interpretable, fácil de entender\n",
    "\n",
    "**Desventajas:** Muy difíl de que se obtenga un buen desempeño predictivo (presume una relación lineal entre cada una de las caracteristicas y la respuesta.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forma de la regresión lineal\n",
    "\n",
    "$y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n$\n",
    "\n",
    "- $y$ es la respuesta\n",
    "- $\\beta_0$ es el intercepto\n",
    "- $\\beta_1$ es el coeficiente para $x_1$ (primera característica)\n",
    "- $\\beta_n$ es el coeficiente para $x_n$ (caracteristica n-esima)\n",
    "\n",
    "En este caso en particular:\n",
    "\n",
    "$y = \\beta_0 + \\beta_1 \\times TV + \\beta_2 \\times Radio + \\beta_3 \\times Newspaper$\n",
    "\n",
    "Los valores $\\beta$ son llamados **coeficientes del modelo**. Estos valores se \"aprenden\" durante el paso de ajuste del modelo utilizando el criterio \"mínimos cuadrados\". Después, el modelo \"entrenado\" se puede usar para hacer pronósticos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando X e y usando pandas\n",
    "\n",
    "- scikit-learn espera que X (matriz de caracteristicas) e y (vector de respuesta) sean arreglos Numpy.\n",
    "- Sin embargo, pandas está construido sobre Numpy.\n",
    "- Por lo tanto, X puede ser un DataFrame de pandas e y pandas Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas=list(data.columns.values)\n",
    "print (columnas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una lista de Python con los nombre de las caracteristicas\n",
    "feature_cols = ['TV', 'Radio', 'Newspaper']\n",
    "\n",
    "# Use esta lista para seleccionar el subconjunto del DataFrame original\n",
    "X = data[feature_cols]\n",
    "\n",
    "# Comando equivalente para hacerlo en una linea\n",
    "#X = data[['TV', 'radio', 'newspaper']]\n",
    "\n",
    "# Tambien se puede usar la lista columnas\n",
    "#X = data[columnas[:len(columnas)]]\n",
    "\n",
    "# Imprime las 5 primeras filas\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chequea el tipo y forma de X\n",
    "print(type(X))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecciona una \"Serie\" del DataFrame\n",
    "y = data['Sales']\n",
    "\n",
    "# Comando equivalente que funciona si no hay espacios en la columna nombre\n",
    "#y = data.Sales\n",
    "\n",
    "# Se puede usar tambien la ultima columna\n",
    "#y = data[columnas[-1]]\n",
    "\n",
    "# Imprime los 5 primeros valores\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chequea el tipo y forma de \"y\"\n",
    "print(type(y))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Particionando X e y en conjuntos de entrenamiento y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por defecto el particionamiento es 75% entrenamiento y 25% test\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión lineal en scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar el modelo\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "\n",
    "# Instanciarlo (alpha siempre debe ser un numero positivo)\n",
    "linreg = LinearRegression()\n",
    "ridge=Ridge(alpha=0.00001)\n",
    "lasso = Lasso(alpha=0.00001)\n",
    "\n",
    "# Ajustar el modelo a los datos de entrenamiento (aprender los coeficientes)\n",
    "linreg.fit(X_train, y_train)\n",
    "ridge.fit(X_train, y_train)\n",
    "lasso.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretando los coeficientes del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprime el intercepto y los coeficientes de la Regresión lineal\n",
    "print(linreg.intercept_)\n",
    "print(linreg.coef_)\n",
    "\n",
    "# Imprime el intercepto y los coeficientes de la Regresión Ridge\n",
    "print(ridge.intercept_)\n",
    "print(ridge.coef_)\n",
    "\n",
    "# Imprime el intercepto y los coeficientes de la Regresión LASSO\n",
    "print(lasso.intercept_)\n",
    "print(lasso.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"parea\" los nombres con sus coeficientes\n",
    "list(zip(feature_cols, linreg.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y = 2.88 + 0.0466 \\times TV + 0.179 \\times Radio + 0.00345 \\times Newspaper$$\n",
    "\n",
    "¿Cómo se interpreta el **coeficientes TV** (0.0466)?\n",
    "\n",
    "- Para un monto dado de gasto de advertising de Radio y Newspaper, **una \"unidad\" de incremente en el gasto de advertising en TV** se asocia **0.0466 \"unidades\" de incremento en Ventas**.\n",
    "- O más claramente: \n",
    "- Para un monto dado de gasto de advertising de Radio y Newspaper, **gastar 1000 adicionales en advertising en TV** se asocia con **un incremento de 46.6 unidades en ventas**.\n",
    "\n",
    "Importante:\n",
    "- No confundir **asociación** con **causalidad**. \n",
    "- Si un incremento en gastos de advertising se asocia con un **decremento** en ventas, $\\beta_1$ sería negativo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizando predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizando predicciones con el conjunto de test\n",
    "y_pred = linreg.predict(X_test)\n",
    "y_pred2 = ridge.predict(X_test)\n",
    "y_pred3 = lasso.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué falta ahora? Revisar la **calidad** de ésta predicción.\n",
    "Necesitamos una **métrica de desempeño** para comparar las predicciones con los valores reales!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas de evaluación de modelos para regresión\n",
    "\n",
    "Las medidas de desempeño o evaluación para problemas de clasificación, tales como **accuracy** o **F1-score**, no sirven para problemas de regresión.\n",
    "A continuación se presentan **tres metricas de evaluación**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir valores reales y predichos\n",
    "true = [100, 50, 30, 20]\n",
    "pred = [90, 50, 50, 30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean Absolute Error** (MAE) es la media de las diferencias absolutas entre el valor real y el predicho:\n",
    "\n",
    "$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular el MAE a mano\n",
    "print(abs(10 + 0 + 20 + 10)/4.)\n",
    "\n",
    "# Calcular MAE con scikit-learn\n",
    "from sklearn import metrics\n",
    "print(metrics.mean_absolute_error(true, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean Squared Error** (MSE) es la media de los errores al cuadrado:\n",
    "\n",
    "$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A mano\n",
    "print((10**2 + 0**2 + 20**2 + 10**2)/4.)\n",
    "\n",
    "# Con scikit-learn\n",
    "print(metrics.mean_squared_error(true, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Root Mean Squared Error** (RMSE) la raíz de MSE:\n",
    "\n",
    "$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A \"mano\"\n",
    "import numpy as np\n",
    "print(np.sqrt((10**2 + 0**2 + 20**2 + 10**2)/4.))\n",
    "\n",
    "# Con scikit-learn\n",
    "print(np.sqrt(metrics.mean_squared_error(true, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparación de estas métricas:\n",
    "\n",
    "- **MAE** es el más fácil de entender, porque es el error promedio.\n",
    "- **MSE** más popular que el anterior, porque penaliza los errores grandes.\n",
    "- **RMSE** más interpretable que el anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computar el RMSE para computar las predicciones de Ventas (primer ejemplo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print(np.sqrt(metrics.mean_squared_error(y_test, y_pred2)))\n",
    "print(np.sqrt(metrics.mean_squared_error(y_test, y_pred3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mejores Parametros para LASSO (tambien aplicable a Ridge Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mejor_resultado=float('Inf')\n",
    "\n",
    "alpha_lasso = [1e-40,1e-35,1e-30, 1e-20, 1e-15, 1e-10, 1e-8, 1e-5,1e-4, 1e-3,1e-2, 1, 5, 10,100,1000,10000] \n",
    "for alpha in alpha_lasso:\n",
    "    lasso = Lasso(alpha=alpha,normalize=True)\n",
    "    lasso.fit(X_train, y_train)\n",
    "    y_pred3 = lasso.predict(X_test)\n",
    "    RMSE=np.sqrt(metrics.mean_squared_error(y_test, y_pred3))\n",
    "    if RMSE<mejor_resultado:\n",
    "        mejor_resultado=RMSE\n",
    "        mejoralpha=alpha\n",
    "print(\"Mejor resultado: \",mejor_resultado)\n",
    "print(\"Mejor alpha: \",mejoralpha)\n",
    "lasso = Lasso(alpha=mejoralpha,normalize=True)\n",
    "lasso.fit(X_train, y_train)\n",
    "print(lasso.intercept_)\n",
    "print(lasso.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de Características\n",
    "\n",
    "¿**Pertenece** la variable **Newspaper** a nuestro modelo? ¿Agrega valor? En otras palabras, ¿aumenta la calidad de las predicciones?\n",
    "\n",
    "**Removamosla** y calculemos el resultado RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creemos una lista de Python para las variables a utilizar\n",
    "feature_cols = ['TV', 'Radio']\n",
    "\n",
    "# Usar la lista para seleccionar las variables del DataFrame\n",
    "X = data[feature_cols]\n",
    "\n",
    "# Seleccionar una Series para y\n",
    "y = data.Sales\n",
    "\n",
    "# Particionar para entrenamiento y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# Ajustar el modelo (\"Aprender\" los coeficientes)\n",
    "linreg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Realizar las predicciones con los datos de test\n",
    "y_pred1 = linreg.predict(X_test)\n",
    "\n",
    "\n",
    "# Computar el RMSE con las predicciones\n",
    "print(np.sqrt(metrics.mean_squared_error(y_test, y_pred1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El RMSE **decrece** cuando se remueve la característica Newspaper del modelo. \n",
    "Por lo tanto, es probable que esta variable NO sea útil para predecir Sales, y debería removerse del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manera alternativa para elegir las mejores caracteristicas\n",
    "- Sklearn seleccion caracteristicas: [link](http://scikit-learn.org/stable/modules/feature_selection.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Creemos una lista de Python para las variables a utilizar\n",
    "feature_cols = ['TV', 'Radio','Newspaper']\n",
    "\n",
    "# Usar la lista para seleccionar las variables del DataFrame\n",
    "X = data[feature_cols]\n",
    "\n",
    "# Seleccionar una Series para y\n",
    "y = data.Sales\n",
    "\n",
    "# Particionar para entrenamiento y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "#Recursive Feature Elimination\n",
    "rfe = RFE(estimator=linreg, n_features_to_select=2)\n",
    "\n",
    "\n",
    "# Ajustar el modelo (\"Aprender\" los coeficientes)\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "#Variables eliminadas\n",
    "print(list(data.columns.values)[:3])\n",
    "print(rfe.support_) \n",
    "\n",
    "# Realizar las predicciones con los datos de test\n",
    "y_pred1 = rfe.predict(X_test)\n",
    "\n",
    "\n",
    "# Computar el RMSE con las predicciones\n",
    "print(np.sqrt(metrics.mean_squared_error(y_test, y_pred1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "exp_LR=[]\n",
    "for i in range(10):\n",
    "    resultados_linreg=[]\n",
    "    #X=np.array(X)\n",
    "\n",
    "    kf = KFold(n_splits=5,shuffle=True)\n",
    "    for k, (train_index, test_index) in enumerate(kf.split(X, y)):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        linreg=LinearRegression()\n",
    "        linreg.fit(X_train,y_train)\n",
    "        y_pred2=linreg.predict(X_test)\n",
    "        resultados_linreg.append(mean_squared_error(y_test,y_pred2))\n",
    "    print (\"Resultado LinReg: \",resultados_linreg)\n",
    "    print (\"Promedio LinReg: \",np.mean(resultados_linreg))\n",
    "    exp_LR.append(np.mean(resultados_linreg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabajo en Clases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Cycle Power Plant Data Set\n",
    "\n",
    "Para predecir el Energy Output(EP) de la planta.\n",
    "Parametros entregados :\n",
    "1. Ambient Temperature (AT)\n",
    "2. Exhaust Vaucum (V)\n",
    "3. Ambient Pressure (AP)\n",
    "4. Relative Humidity (RH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>V</th>\n",
       "      <th>AP</th>\n",
       "      <th>RH</th>\n",
       "      <th>EP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.96</td>\n",
       "      <td>41.76</td>\n",
       "      <td>1024.07</td>\n",
       "      <td>73.17</td>\n",
       "      <td>463.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25.18</td>\n",
       "      <td>62.96</td>\n",
       "      <td>1020.04</td>\n",
       "      <td>59.08</td>\n",
       "      <td>444.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.11</td>\n",
       "      <td>39.40</td>\n",
       "      <td>1012.16</td>\n",
       "      <td>92.14</td>\n",
       "      <td>488.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.86</td>\n",
       "      <td>57.32</td>\n",
       "      <td>1010.24</td>\n",
       "      <td>76.64</td>\n",
       "      <td>446.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.82</td>\n",
       "      <td>37.50</td>\n",
       "      <td>1009.23</td>\n",
       "      <td>96.62</td>\n",
       "      <td>473.90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      AT      V       AP     RH      EP\n",
       "0  14.96  41.76  1024.07  73.17  463.26\n",
       "1  25.18  62.96  1020.04  59.08  444.37\n",
       "2   5.11  39.40  1012.16  92.14  488.56\n",
       "3  20.86  57.32  1010.24  76.64  446.48\n",
       "4  10.82  37.50  1009.23  96.62  473.90"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import Lasso\n",
    "from numpy import arange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv(\"powerplant.csv\",header=0)\n",
    "# Muestra las 5 primeras filas\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47840, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenar Modelo Regresion Lineal, Ridge y Lasso. (Debe buscar el parametro ideal de alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data.iloc[:,:-1], data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model = Lasso()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = RepeatedKFold(n_splits = 10, n_repeats = 3, random_state = 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = dict()\n",
    "#grid['alpha'] = arange(0.00001, , 0.01)\n",
    "grid['alpha'] = np.array([0.0001,0.001,0.01,0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GridSearchCV(lasso_model, grid, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 4.557428577594684\n"
     ]
    }
   ],
   "source": [
    "print(f'RMSE: {-1*results.best_score_}')\n",
    "#$  4.557611602159267"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: {'alpha': 0.01}\n"
     ]
    }
   ],
   "source": [
    "print(f'Config: {results.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Lasso(alpha = results.best_params_['alpha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.97739393, -0.23398702,  0.06182598, -0.15796772])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.01)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(y, y_pred)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graficar salida de los 3 modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcular el Coeficiente de Correlación de Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#import scipy.stats\n",
    "#x = np.arange(10, 20)\n",
    "#y = np.array([2, 1, 4, 5, 8, 12, 18, 25, 96, 48])\n",
    "#scipy.stats.pearsonr(x, y)    # Pearson's r\n",
    "#(0.7586402890911869, 0.010964341301680832)\n",
    "#scipy.stats.spearmanr(x, y)   # Spearman's rho\n",
    "#SpearmanrResult(correlation=0.9757575757575757, pvalue=1.4675461874042197e-06)\n",
    "#scipy.stats.kendalltau(x, y)  # Kendall's tau\n",
    "#KendalltauResult(correlation=0.911111111111111, pvalue=2.9761904761904762e-05)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "name": "05.06-Linear-Regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
